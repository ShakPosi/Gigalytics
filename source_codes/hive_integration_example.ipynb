{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nos.environ[\"SPARK_HOME\"] = \"/home/osboxes/spark/spark-2.4.0-bin-hadoop2.7/\"\\n\\nimport findspark\\nfindspark.init()\\nfindspark.find()\\nimport pyspark\\nfrom pyspark import SparkContext, SparkConf\\nfrom pyspark.sql import SparkSession\\n\\nfrom pyspark.sql.functions import levenshtein \\n#import pyspark.sql.HiveContext as HiveContext\\nconf = pyspark.SparkConf().setAppName(\\'appName\\').setMaster(\\'local\\')\\n#conf = pyspark.SparkConf().setAppName(\\'appName\\').setMaster(\\'spark://192.168.11.128:8080\\')\\n\\nsc = pyspark.SparkContext(conf=conf)\\nspark = SparkSession(sc)\\n\\n#spark.enableHiveSupport()\\n#import spark.sql.hive.HiveContext as HiveContext\\n#HiveContext sqlContext = spark.sql.hive.HiveContext(sc.sc())\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/osboxes/spark/spark-2.4.0-bin-hadoop2.7/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import levenshtein \n",
    "#import pyspark.sql.HiveContext as HiveContext\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "#conf = pyspark.SparkConf().setAppName('appName').setMaster('spark://192.168.11.128:8080')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#spark.enableHiveSupport()\n",
    "#import spark.sql.hive.HiveContext as HiveContext\n",
    "#HiveContext sqlContext = spark.sql.hive.HiveContext(sc.sc())\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weeklyTop200 = spark.read.csv('../datasets/spotify_top_200_weekly.csv', header=True)\n",
    "df_trackDetails = spark.read.csv('../datasets/spotify_track_details.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weeklyTop200 = df_weeklyTop200.withColumnRenamed(\"Track Name\",\"Track_Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weeklyTop200.write.mode('Overwrite').saveAsTable('spotify_top_200_weekly')\n",
    "df_trackDetails.write.mode('Overwrite').saveAsTable('spotify_track_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------------+----------------+-----------------+--------------+--------------------+------------------+------------------+----------+\n",
      "|     track_spotifyID|    artist_spotifyID|         artist_mbid|  artist_name|           track_url|track_popularity|track_duration_ms|track_is_local|             albumID|album_track_number|album_release_date|album_type|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------------+----------------+-----------------+--------------+--------------------+------------------+------------------+----------+\n",
      "|2TIlqbIneP0ZY1O0E...|4GNC7GD6oZMSxPGyX...|526aab94-697f-44a...|Lewis Capaldi|https://open.spot...|              82|           182160|         False|0NVQ9k3wKmuK6T02l...|                 3|        2018-11-08|    single|\n",
      "|4kV4N9D1iKVxx1KLv...|66CXWjxzNUsdJxJ2J...|f4fdbb4c-e4b7-47a...|Ariana Grande|https://open.spot...|              81|           190440|         False|2fYhqwDWXjbpjaIJP...|                12|        2019-02-08|     album|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------------+----------------+-----------------+--------------+--------------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from spotify_track_details limit 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_spotifyID',\n",
       " 'artist_spotifyID',\n",
       " 'artist_mbid',\n",
       " 'artist_name',\n",
       " 'track_url',\n",
       " 'track_popularity',\n",
       " 'track_duration_ms',\n",
       " 'track_is_local',\n",
       " 'albumID',\n",
       " 'album_track_number',\n",
       " 'album_release_date',\n",
       " 'album_type']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trackDetails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3057"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = spark.sql('select * from spotify_track_details')\n",
    "rs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from spotify_top_200_weekly stw \\\n",
    "where Position > 0  and Position <= 10 and region = 'gb'\")\\\n",
    ".createOrReplaceTempView('spotify_top_10_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count('position'), Artist from spotify_top_10_weekly stw \\\n",
    "where stw.region = 'gb' group by Artist\")\\\n",
    ".createOrReplaceTempView('vw_unique_uk_artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(artist_mbid), artist_spotifyid , artist_mbid,  artist_name \\\n",
    "from spotify_track_details std where artist_mbid <> '' \\\n",
    "group by artist_spotifyid , artist_mbid,  artist_name\") \\\n",
    ".createOrReplaceTempView('vw_unique_artist_mbid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs = spark.sql(\"select * from vw_unique_uk_artist a left join vw_unique_artist_mbid b on a.\" + \"\"\"Artist\"\"\" + \" = b.artist_name where b.artist_spotifyid is null order by 2\")\n",
    "\n",
    "rs = spark.sql(\"select * from vw_unique_uk_artist a \\\n",
    "left join vw_unique_artist_mbid b on a.Artist = b.artist_name \\\n",
    "where b.artist_spotifyid is not null order by 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read saved tables from spark datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('spark-warehouse/weeklytop200/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('spotify_top_200_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from spotify_top_200_weekly stw \\\n",
    "where Position > 0  and Position <= 10 and region = 'gb'\").createOrReplaceTempView('spotify_top_10_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(1) from spotify_top_10_weekly').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table and load data from csv\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spotify_track_details ( \\\n",
    "    track_spotifyid STRING, \\\n",
    "    artist_spotifyid STRING,\\\n",
    "    artist_mbid STRING,\\\n",
    "    artist_name STRING,\\\n",
    "    track_url STRING,\\\n",
    "    track_popularity STRING,\\\n",
    "    track_duration_ms STRING,\\\n",
    "    track_is_local STRING,\\\n",
    "    albumid STRING,\\\n",
    "    album_track_number STRING,\\\n",
    "    album_release_date STRING,\\\n",
    "    album_type STRING) USING hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '../datasets/spotify_track_details.csv' INTO TABLE spotify_track_details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select track_spotifyid,artist_spotifyid, artist_mbid, track_popularity from spotify_track_details').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "# Loading data from a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Specifying dataframe column data types on read\n",
    "jdbcDF3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\") \\\n",
    "    .load()\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "jdbcDF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .save()\n",
    "\n",
    "jdbcDF2.write \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Specifying create table column data types on write\n",
    "jdbcDF.write \\\n",
    "    .option(\"createTableColumnTypes\", \"name CHAR(64), comments VARCHAR(1024)\") \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "dataframe = spark.read.format('jdbc').options(\n",
    "    url = \"jdbc:postgresql://localhost/public?user=postgres&password=postgres\",\n",
    "    database='public',\n",
    "    dbtable='spotify_top_200_weekly'\n",
    ").load()\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('CREATE TABLE spotify_top_200_weekly (Position varchar(32767) NULL,spotify_id varchar(32767) NULL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc, sparkSession=spark, jsqlContext=None)\n",
    "url = 'jdbc:postgresql://localhost/gig_stg?user=postgres&password=postgres'\n",
    "df = sqlContext.read.jdbc(url=url, table='spotify_top_weekly_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/osboxes/spark/spark-2.4.0-bin-hadoop2.7/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import levenshtein \n",
    "\n",
    "#conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "#conf = pyspark.SparkConf().setAppName('appName').setMaster('spark://192.168.11.128:8080')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"/home/osboxes/postgresql-42.2.11.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "#${url}\tconnection URL\tjdbc:postgresql://osboxes:5432/postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://osboxes:5432/postgres\")\n",
    "    .option('driver', '')\n",
    "    .option(\"dbtable\", \"spotify_top_200_weekly\")\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', '/home/osboxes/postgresql-42.2.11.jar').getOrCreate()\n",
    "url = 'jdbc:postgresql://108.167.159.189:5432/postgres'\n",
    "properties = {'user': 'postgres', 'password': 'postgres'}\n",
    "df = spark.read.jdbc(url=url, table='spotify_top_200_weekly', properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
